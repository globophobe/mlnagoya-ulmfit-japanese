import aspect from '@mdx-deck/themes/aspect';
import { syntaxHighlighter } from 'mdx-deck/themes'
import theme from './theme';
import arex from './assets/arex.jpg';
import jh from './assets/jeremy-howard.png';
import ulmfit from './assets/ulmfit.png';
import ulmfit1 from './assets/ulmfit1.png';
import ulmfit2 from './assets/ulmfit2.png';
import ulmfit3 from './assets/ulmfit3.png';
import ratings from './assets/ratings.png';

export const themes = [ aspect, syntaxHighlighter, theme ];

# [Fast.ai](http://fast.ai) Lesson 4
# Natural Language Processing with ULMFiT
## ULMFiTで自然言語処理

---

# These slides are at Connpass. So you can check them out.
## スライドはConnpassにアップされています。 是非みてください。

---

<SplitRight>

<img src={arex} />

# My username on Connpass is "globophobe".
## Connpassのユーザ名は「globophobe」です。

</SplitRight>

---
# A little while ago, I finished Fast.ai lesson four.
## 少し前、Fast.aiレッスン4を終えました。

---

# Coincidentally, Fast.ai has documentation to use [Microsoft Data Science Virtual Machines](https://course.fast.ai/start_azure.html).
## 偶然にも、Fast.aiにはMicrosoft Data Science Virtual Machinesを使用するためのドキュメントがあります。

---

# Azure Machine Learning seems useful.
## Azure Machine Learningは役に立つだそうです。

---

# To continue, Fast.ai is a free deep learning Python library and curriculum.
## それでは、Fast.aiの話を続けって、無料の機械学習Pythonライブラリとカリキュラムです。

---

<SplitRight>

<img src={jh} />

# The teacher was Kaggle #1, and its president. 
## 先生はKaggleの１位、そしてKaggleの組織の会長でした。

</SplitRight>

---

# [Lesson 1](https://course.fast.ai/videos/?lesson=1) and [lesson 2](https://course.fast.ai/videos/?lesson=2) explain how to create a CNN with ResNet for transfer learning.
## レッスン1と2は、簡単に転送学習用のResNetを使用してCNNを作成方法を説明します。

---

# [Lesson 3](https://course.fast.ai/videos/?lesson=3) explains in more detail how to use Fast.ai for image processing.
## レッスン3では、Fast.aiライブラリを使用して、画像処理をする方法についてさらに詳しく説明します。

---

# [Lesson 4](https://course.fast.ai/videos/?lesson=4) explains how to use Fast.ai for natural language processing (NLP). 
## レッスン4では、自然言語処理のために、Fasti.aiを利用する方法について説明します。

---

# Fast.ai uses transfer learning for NLP.
## Fast.aiは自然言語処理のために転送学習を使用します

---

# The paper "[Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)" was published on arXiv.org in January 2018.
## 2018年1月にarXiv.orgで論文が公開されました。

---

# According to the paper, because the ULMFiT model doesn’t have to learn from scratch...
## 論文によると、ULMFiTモデルはゼロから学習することが必要ないので、

---

# ...it can generally reach higher accuracy with much less data and compute time.
## 一般に、少ないデータの量と計算時間で、より高い精度に達することが出来ます。

---

# ULMFit was state of the art (SOTA) for sentiment analysis on the Internet Movie Database (IMDB) until recently. 
## ULMFitは最近まで、IMDBデータセットで、感情分析の最先端でした。

---

# Presently, XLNet is number 1.
## 現在、XLNetは1位です。

```
           XLNet            96.21              
           ULMFiT           95.4
```
---

# Related to, Japanese sentiment analysis...
## 日本の感情分析に関する、

---

# A while ago, "[An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese](https://arxiv.org/abs/1905.09642)" was published on arXiv.org
## 少し前、ULMFiTと日本語自然言語処理に関する論文がarXiv.orgで公開されました。

---

# That paper was published by researchers of the company ExaWizards in Tokyo.
## その論文は、東京のExaWizards株式会社の研究者によって発表されました。

---

# The ELMo, ULMFiT, and BERT language models were compared.
## ELMo、ULMFiT、そしてBERTという言語モデルは比較されました。

---

# Specifically, the research was about pre-training a language model in an unsupervised manner...
## 具体的に、言語モデルは監督されないままで事前トレーニングして、

---

# ...and then fine-tuning it on a domain-specific dataset.
## そして、ドメイン固有のデータセットを利用して、微調整する事です。

---

<SplitRight>

<img src={ulmfit} />

# Wikipedia is used for the pretrained model.
## ウィキペディアは事前トレーニングモードに使用されます。

</SplitRight>

---

# The datasets used were the [Japanese Rakuten product review binary and 5 class datasets](https://github.com/zhangxiangxiao/glyph)...
## 使用されたデータセットは、日本の楽天製品レビューデータセットで、

---

# ...as well as the [Japanese Yahoo movie review dataset](https://github.com/dennybritz/sentiment-analysis).
## 日本のYahoo映画レビューデータセットでした。

---

# The researchers said the Yahoo movie dataset "better represents real life/practical situations".
## Yahooの映画データセットが「実際の普段に解決するのが必要の問題」と言われました。

---

# It seems the ExaWizards code and models will be released sometime after the ACL 2019 Annual Meeting.
## コードとモデルは、ACL 2019年次総会の後にリリースされる予定です。

---

# However, they have not been released yet.
## しかし、まだリリースされていません。 

---

# I thought I would try to make a classifier using the Yahoo movie dataset.
## Yahooの映画データセットで分類子を作成しようと思いました。

---

# Fast.ai recommends the Wikitext-103 pretrained model for transfer learning.
## Wikitext 103の事前学習済みモデルは、転移学習に勧められています。

---

# Unfortunately, Fast.ai provides Wikitext-103 for English only.
## 残念ながら、Fast.aiは英語のみのWikitext-103を提供しています。

---

# At this time, there seem to be no pretrained models for Wikitext-103 for Japanese.
## 現在、日本語版Wikitext-103の事前トレーニングモデルはないようです。

---

# There is a [ulmfit-multilingual](https://github.com/tsurushun/ulmfit-multilingual/tree/japanese) repo for Japanese on github.
## githubには、日本語用のulmfit-multilingual gitフォークがあります。

---

# It is for an older version of Fast.ai
## Fast.aiの少し古いバージョン用です。

---

# However, I could update the repo, and train Wikitext-2. 
## しかし、レポジトリを更新し、Wikitext-2をトレーニング出来ました。

---

# Wikitext-2 is about 50 times smaller than Wikitext-103.
## Wikitext-2は、Wikitext-103より約50倍小さいです。

---

# Unfortunately, I didn't have enough RAM to train Wikitext-103.
## 残念ながら、Wikitext-103をトレーニングするためRAMが不十分でした。

---

# However, I thought Wikitext-2 would be enough to train a simple classifier.
## しかし、単純な分類子を作成するにはWikitext-2で十分だと思いました。

---

<SplitRight>

<img src={ulmfit1} />

# Step 1 complete.
## ステップ1完成です。

</SplitRight>


---

# Next, fine-tuning with the Yahoo movie dataset.
## 次はYahoo映画データセットを使用して微調整する事です。

---

# There are 91,196 reviews of 64 movies in the Yahoo movie dataset:
## Yahooデータセットには64の映画の91,196件のレビューがあります

---

# The next slide is the first review in the dataset.
## 次のスライドはデータセットの一番最初のレビューです。

---

```
movieName: おくりびと		

title: ♡ヒロスエのパンツ♡

text:	
を見れたのが意外な収穫でした♡\n\n邦画もたまには良いかな。

rating: 3
```

---

<SplitRight>

<img src={ratings} />

# There are 5 ratings, and the classes are unbalanced.
## 5つの評価があり、クラスは不均衡です。

</SplitRight>

---

# I assumed good reviews would be longer than bad reviews.
## 良いレビューは悪いレビューよりも長いと思いました。

---

# In fact, the lengths were not so different.
## 実際、長さはそれほど違いはなかったです。

---

## Review lengths レビューの長さ

```
		rating  mean    std     min  max			
		1       245.2   285.5  	1    2104
		2       324.3   317.7  	1    2093
		3       344.4   318.7  	1    2067
		4       365.3   334.6  	1    2317
		5       343.9   358.9  	0    2242
```

---

# The English example in the Fast.ai docs, uses 1000 reviews from the IMDB dataset.
## Fast.aiドキュメントの英語の例は、IMDBデータセットの1000件のレビューを使用しています。

---

# Because I used Wikitext-2, I decided to use 10,000 reviews from the Yahoo dataset.
## Wikitext-2を使用したため、Yahooデータセットから10,000件のレビューを使用する事にしました。

---

```
  learn = language_model_learner(
 	  data_lm, AWD_LSTM, 
	  pretrained_fnames=('qrnn_wt-2', 'itos_wt-2'), 
 	  drop_mult=0.5
  )
  learn.fit_one_cycle(1, 1e-2)

  epoch train_loss  valid_loss  accuracy  time
  0     4.590764    4.487607    0.243777  1:20:02
```

---

```
  learn.unfreeze()
  learn.fit_one_cycle(1, 1e-3)

  epoch train_loss  valid_loss  accuracy  time
  0     4.272850    4.243399    0.272256  2:06:08

  learn.save_encoder('ft_enc')
```

---

<SplitRight>

<img src={ulmfit2} />

# Step 2 complete.
## ステップ2完成です。

</SplitRight>

---

# Next, training the classifier.
## 次に、分類器をトレーニングする事です。

---

```
classifier = text_classifier_learner(			
	data_clas, AWD_LSTM, 
	pretrained=False, 
	drop_mult=0.5)
classifier.load_pretrained(
	wgts_fname=path/'models'/'qrnn_wt-2.pth', 
	itos_fname=path/'models'/'itos_wt-2.pkl', 
	strict=False)
classifier.load_encoder('ft_enc')
```

---

```
 classifier.fit_one_cycle(1, 1e-2)

 epoch train_loss  valid_loss  accuracy  time
 0     0.577836    0.508297    0.756122  1:00:52
```

---

```
 classifier.freeze_to(-2)
 classifier.fit_one_cycle(1, slice(5e-3/2., 5e-3))

 epoch train_loss  valid_loss  accuracy  time
 0     0.429069    0.360367    0.837581  1:10:35
```

---

# Accuracy is almost the same as the 1000 IMDB sample in the Fast.ai docs.
## 精度は、Fast.aiドキュメントの1000 IMDBサンプルとほぼ同じです。


---

<SplitRight>

<img src={ulmfit3} />

# Step 3 complete.
## ステップ3完成です。

</SplitRight>

---

# Success 成功

```
 classifier.predict("すごく面白い映画です。")			

 (Category positive, 
	tensor(1), 
	tensor([3.5251e-05, 9.9996e-01]))
```

---

# To improve, I would like to try using Japanese Wikitext-103...
## 改善するためには、日本語Wikitext-103を使用したいと思います、

---

# ...and, improve tokenization.
## そして、トークン化を改善したいです。

---

# By default, Fast.ai uses [spacy](https://spacy.io/) for tokenization.
## デフォルトでは、Fast.aiはトークン化にspacyを使用します。

---

# However, the github repo has a TODO: "[handle tokenization of Chinese, Japanese, Korean](https://github.com/fastai/fastai/blob/4a7c6b6a21152de4af86cee1d48e844f509f6625/courses/dl2/imdb_scripts/create_toks.py#L48)".
## しかし、githubリポジトリにはTODOがあります：「中国語、日本語、韓国語のトークン化を処理する」

---

# Tokenization of Japanese is not officially supported.
## 日本語のトークン化は公式にはサポートされていません。

---

# The ulmfit-multilingual repo uses SentencePiece, so I used that.
## ulmfit-multilingualリポジトリはSentencePieceを使用するので、それを使用しました。

---

# But, I was not satisfied with the tokenization.
## しかし、トークン化には満足していませんでした。

---

# I tried other tokenizers, including MeCab, Janome, Rakuten MA, SudachiPy, and Nagisa.
## 他のトークナイザーを試しました。

---

# The Exawizards researchers used a MeCab tokenizer.
## Exawizardsの研究者はMeCabトークナイザーを使用しました。

---

# When Exawizards release their code I would like to look at their tokenization method.
## Exawizardsがコードをリリースする時、トークン化の方法を見たいと思います。

---
# That’s all. Thanks for listening.
## 以上です。ご清聴ありがとうございます。
